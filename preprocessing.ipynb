{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```markdown\n",
    "### GPU Configuration and Memory Management\n",
    "\n",
    "This code configures TensorFlow to manage GPU resources effectively. It performs the following tasks:\n",
    "\n",
    "1. **List Available GPUs**:\n",
    "    - Retrieves a list of physical GPU devices available on the system using `tf.config.experimental.list_physical_devices('GPU')`.\n",
    "\n",
    "2. **Set Memory Growth**:\n",
    "    - Enables memory growth for each GPU using `tf.config.experimental.set_memory_growth`. This ensures that TensorFlow only allocates GPU memory as needed, rather than reserving all available memory upfront.\n",
    "\n",
    "3. **Limit GPU Memory**:\n",
    "    - Configures virtual devices with a memory limit using `tf.config.experimental.VirtualDeviceConfiguration`. This allows TensorFlow to allocate only a specified amount of GPU memory.\n",
    "\n",
    "4. **Verification**:\n",
    "    - Prints the number of physical and logical GPUs detected to confirm the configuration.\n",
    "\n",
    "This setup is particularly useful when working with multiple GPUs or when running TensorFlow on shared GPU resources.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Get list of available GPUs\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "\n",
    "if gpus:\n",
    "    try:\n",
    "        # Set memory growth to False for all GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, False)\n",
    "\n",
    "        # Limit GPU memory growth to the fraction used by TensorFlow\n",
    "        # Note: This will allocate the entire GPU memory upfront\n",
    "        for gpu in gpus:\n",
    "            physical_device = gpu\n",
    "            tf.config.experimental.set_virtual_device_configuration(\n",
    "                physical_device,\n",
    "                [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=physical_device.memory_limit)]\n",
    "            )\n",
    "\n",
    "        # Verification\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# List all local GPUs detected by TensorFlow\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "\n",
    "# Check if TensorFlow is configured to allocate GPU memory dynamically for each GPU\n",
    "for gpu in gpus:\n",
    "    memory_growth = tf.config.experimental.get_memory_growth(gpu)\n",
    "    print(\"GPU memory growth for {}: {}\".format(gpu.name, memory_growth))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorrt\n",
    "from skimage import io, filters\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import layers\n",
    "gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "for device in gpu_devices:\n",
    "    tf.config.experimental.set_memory_growth(device, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Preprocessing\n",
    " (Currently using ISIC 2017)\n",
    " \n",
    "1. **Gaussian Filter for Noise Reduction**: filters.gaussian is applied to the loaded image with a specified sigma value (sigma=5 in this case) to reduce noise.\n",
    "\n",
    "2. **Gaussian Filter for Smoothing**: Another filters.gaussian filter is applied to the noise-reduced image with a smaller sigma value (sigma=2) to further smooth the image.\n",
    "\n",
    "3. **Standardization**: The smoothed image is standardized by subtracting the mean value of the image and dividing by its standard deviation. This step helps in normalizing the image intensities.\n",
    "\n",
    "4. **Resize**: The standardized image is resized to a target size of (224, 224) using cv2.resize. This is commonly done to ensure that all images fed into the model have the same dimensions.\n",
    "\n",
    "5. **Centering and Normalization**: After resizing, the mean value is subtracted from the resized image, and then the image is divided by twice the standard deviation value. This centers the image around zero and normalizes it. This step ensures that the pixel values are within a certain range, making the model training process more stable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_images(image_paths):\n",
    "    preprocessed_images = []\n",
    "    labels = []\n",
    "    for image_path in image_paths:\n",
    "        if os.path.basename(image_path).startswith('.'):\n",
    "            continue\n",
    "        \n",
    "        # Extract label from the folder name\n",
    "        label = os.path.basename(os.path.dirname(image_path))\n",
    "        \n",
    "        original_image = io.imread(image_path)\n",
    "\n",
    "        # Apply Gaussian filter for noise reduction\n",
    "        noise_reduced_image = filters.gaussian(original_image, sigma=5)\n",
    "\n",
    "        # Apply Gaussian filter for smoothing\n",
    "        smoothed_image = filters.gaussian(noise_reduced_image, sigma=2)\n",
    "\n",
    "        # Standardize the smoothed image\n",
    "        mean_value = np.mean(smoothed_image)\n",
    "        std_value = np.std(smoothed_image)\n",
    "        standardized_image = (smoothed_image - mean_value) / std_value\n",
    "        \n",
    "        # Scale the values to range [0, 1]\n",
    "        standardized_image = (standardized_image - np.min(standardized_image)) / (np.max(standardized_image) - np.min(standardized_image))\n",
    "\n",
    "        # Resize the standardized image\n",
    "        target_size = (224, 224)\n",
    "        resized_image = cv2.resize(standardized_image, target_size, interpolation=cv2.INTER_AREA)\n",
    "\n",
    "        # Center and normalize the resized image\n",
    "        mean_value = np.mean(resized_image)\n",
    "        std_value = np.std(resized_image)\n",
    "        centered_normalized_image = (resized_image - mean_value) / (std_value * 2.0)\n",
    "        \n",
    "        # Scale the values to range [0, 1]\n",
    "        centered_normalized_image = (centered_normalized_image - np.min(centered_normalized_image)) / (np.max(centered_normalized_image) - np.min(centered_normalized_image))\n",
    "\n",
    "        preprocessed_images.append(centered_normalized_image)\n",
    "        labels.append(label)\n",
    "\n",
    "    return preprocessed_images, labels\n",
    "\n",
    "# Set input folders with the path of the melanoma and non melanoma images\n",
    "input_folders = ['cancer 2018/task_2/akiec', \n",
    "                 'cancer 2018/task_2/bcc',\n",
    "                 'cancer 2018/task_2/bkl',\n",
    "                 'cancer 2018/task_2/df',\n",
    "                 'cancer 2018/task_2/mel',\n",
    "                 'cancer 2018/task_2/nv',\n",
    "                 'cancer 2018/task_2/vasc']\n",
    "\n",
    "# Preprocess images\n",
    "all_images = []\n",
    "all_labels = []\n",
    "for folder in input_folders:\n",
    "    image_paths = [os.path.join(folder, filename) for filename in os.listdir(folder)]\n",
    "    preprocessed_images, labels = preprocess_images(image_paths)\n",
    "    all_images.extend(preprocessed_images)\n",
    "    all_labels.extend(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from skimage import io, filters\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from skimage.metrics import structural_similarity, mean_squared_error, peak_signal_noise_ratio\n",
    "# Preprocessing function to generate noisy and reconstructed images\n",
    "def preprocess_single_image(image_path):\n",
    "    original_image = io.imread(image_path)\n",
    "    # Resize the standardized image\n",
    "    target_size = (224, 224)\n",
    "    resized_image = cv2.resize(original_image, target_size, interpolation=cv2.INTER_AREA)\n",
    "\n",
    "    # Apply Gaussian filter for noise reduction\n",
    "    noise_reduced_image = filters.gaussian(resized_image, sigma=5)\n",
    "\n",
    "    # Apply Gaussian filter for smoothing\n",
    "    smoothed_image = filters.gaussian(noise_reduced_image, sigma=2)\n",
    "\n",
    "    # Standardize the smoothed image\n",
    "    mean_value = np.mean(smoothed_image)\n",
    "    std_value = np.std(smoothed_image)\n",
    "    standardized_image = (smoothed_image - mean_value) / std_value\n",
    "\n",
    "    \n",
    "\n",
    "    # Center and normalize the resized image\n",
    "    mean_value = np.mean(standardized_image)\n",
    "    std_value = np.std(standardized_image)\n",
    "    centered_normalized_image = (standardized_image - mean_value) / (std_value * 2.0)\n",
    "\n",
    "    # Scale the values to range [0, 1]\n",
    "    centered_normalized_image = (centered_normalized_image - np.min(centered_normalized_image)) / (np.max(centered_normalized_image) - np.min(centered_normalized_image))\n",
    "\n",
    "    return resized_image, smoothed_image\n",
    "\n",
    "noisy_image, centered_normalized_image = preprocess_single_image('cancer 2017/melanoma/ISIC_ISIC_0000148.jpg')\n",
    "\n",
    "print('ssim',structural_similarity(noisy_image, centered_normalized_image,channel_axis=2))\n",
    "\n",
    "print('mse',mean_squared_error(noisy_image, centered_normalized_image)/1000)\n",
    "\n",
    "print('psnr',peak_signal_noise_ratio(noisy_image, centered_normalized_image))\n",
    "#print(calculate_ssim(noisy_image, centered_normalized_image))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since preprocessing 2000 images takes more than an hour, it's saved into a numpy array, which is loaded later. This way, we only need to perform the preprocessing once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save preprocessed images to a file\n",
    "np.save('all_images_1000.npy', all_images)\n",
    "np.save('all_labels_1000.npy',all_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Later on we will be only loading these array for the images and labels so that it can be used for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the numpy array into all_images and all_labels\n",
    "all_images = np.load('images_aug.npy')\n",
    "all_labels = np.load('labels-aug.npy')\n",
    "print(all_images.shape)\n",
    "print(all_labels.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
