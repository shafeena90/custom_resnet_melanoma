{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Custom ResNet-50 v2 Model Training and Evaluation\n",
    "\n",
    "This notebook demonstrates the process of training and evaluating a custom ResNet-50 v2 model with an additional stage. The workflow includes data preprocessing, model architecture definition, training, evaluation, and visualization of results.\n",
    "\n",
    "## Workflow Overview\n",
    "\n",
    "1. **Data Loading and Preprocessing**:\n",
    "    - Load images and labels from `.npy` files.\n",
    "    - Balance the dataset by augmenting underrepresented classes.\n",
    "    - Split the dataset into training and validation sets.\n",
    "\n",
    "2. **Model Architecture**:\n",
    "    - Define a custom ResNet-50 v2 model with an additional stage (Stage 3.5).\n",
    "    - Use identity and convolutional blocks to build the model.\n",
    "\n",
    "3. **Training**:\n",
    "    - Compile the model with Adam optimizer and sparse categorical cross-entropy loss.\n",
    "    - Train the model using the balanced dataset with learning rate scheduling.\n",
    "\n",
    "4. **Evaluation**:\n",
    "    - Calculate metrics such as accuracy, precision, recall, F1-score, and confusion matrix.\n",
    "    - Visualize training and validation accuracy/loss over epochs.\n",
    "    - Plot the confusion matrix and ROC curves for multi-class classification.\n",
    "\n",
    "5. **Saving Results**:\n",
    "    - Save the trained model and metrics to files for future use.\n",
    "\n",
    "## Key Features\n",
    "\n",
    "- **Data Augmentation**: Random flipping, brightness, and contrast adjustments to improve model generalization.\n",
    "- **Custom Model Architecture**: ResNet-50 v2 with an additional stage for enhanced feature extraction.\n",
    "- **Performance Metrics**: Comprehensive evaluation using multiple metrics and visualizations.\n",
    "- **Model Saving**: Save the trained model and metrics for reproducibility.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading and ResNet-50 v2 Block Definitions\n",
    "\n",
    "This section includes the following steps:\n",
    "\n",
    "1. **Data Loading**:\n",
    "    - Images and labels are loaded from `.npy` files using `numpy`.\n",
    "    - The shapes of the loaded images and labels are printed to verify the data.\n",
    "\n",
    "2. **Identity Block (v2)**:\n",
    "    - Implements the identity block for ResNet-50 v2.\n",
    "    - The block includes:\n",
    "        - Batch normalization and ReLU activation.\n",
    "        - Three convolutional layers with specified filters and kernel sizes.\n",
    "        - A shortcut connection that adds the input to the output of the main path.\n",
    "\n",
    "3. **Convolutional Block (v2)**:\n",
    "    - Implements the convolutional block for ResNet-50 v2.\n",
    "    - The block includes:\n",
    "        - Batch normalization and ReLU activation.\n",
    "        - Three convolutional layers with specified filters and kernel sizes.\n",
    "        - A shortcut connection with a convolutional layer to match dimensions.\n",
    "        - Stride is used to downsample the input.\n",
    "\n",
    "These blocks are essential components of the ResNet-50 v2 architecture, enabling deep feature extraction and efficient gradient flow through the network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from collections import Counter\n",
    "from tensorflow.keras.layers import Conv2D, BatchNormalization, Activation, Add, Input, ZeroPadding2D, MaxPooling2D, AveragePooling2D, Flatten, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Load the images and labels\n",
    "images = np.load('images.npy')\n",
    "labels = np.load('labels.npy')\n",
    "\n",
    "print(\"Images shape:\", images.shape)\n",
    "print(\"Labels shape:\", labels.shape)\n",
    "\n",
    "def identity_block_v2(X, f, filters, stage, block):\n",
    "    # Define name basis\n",
    "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
    "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
    "\n",
    "    # Retrieve Filters\n",
    "    F1, F2, F3 = filters\n",
    "\n",
    "    # Save the input value\n",
    "    X_shortcut = X\n",
    "\n",
    "    # First component of main path\n",
    "    X = BatchNormalization(axis=3, name=bn_name_base + '2a')(X)\n",
    "    X = Activation('relu')(X)\n",
    "    X = Conv2D(F1, (1, 1), strides=(1, 1), padding='valid', name=conv_name_base + '2a')(X)\n",
    "\n",
    "    # Second component of main path\n",
    "    X = BatchNormalization(axis=3, name=bn_name_base + '2b')(X)\n",
    "    X = Activation('relu')(X)\n",
    "    X = Conv2D(F2, (f, f), strides=(1, 1), padding='same', name=conv_name_base + '2b')(X)\n",
    "\n",
    "    # Third component of main path\n",
    "    X = BatchNormalization(axis=3, name=bn_name_base + '2c')(X)\n",
    "    X = Activation('relu')(X)\n",
    "    X = Conv2D(F3, (1, 1), strides=(1, 1), padding='valid', name=conv_name_base + '2c')(X)\n",
    "\n",
    "    # Final step: Add shortcut value to main path\n",
    "    X = Add()([X, X_shortcut])\n",
    "\n",
    "    return X\n",
    "\n",
    "def convolutional_block_v2(X, f, filters, stage, block, s=2):\n",
    "    # Define name basis\n",
    "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
    "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
    "\n",
    "    # Retrieve Filters\n",
    "    F1, F2, F3 = filters\n",
    "\n",
    "    # Save the input value\n",
    "    X_shortcut = X\n",
    "\n",
    "    # First component of main path\n",
    "    X = BatchNormalization(axis=3, name=bn_name_base + '2a')(X)\n",
    "    X = Activation('relu')(X)\n",
    "    X = Conv2D(F1, (1, 1), strides=(s, s), padding='valid', name=conv_name_base + '2a')(X)\n",
    "\n",
    "    # Second component of main path\n",
    "    X = BatchNormalization(axis=3, name=bn_name_base + '2b')(X)\n",
    "    X = Activation('relu')(X)\n",
    "    X = Conv2D(F2, (f, f), strides=(1, 1), padding='same', name=conv_name_base + '2b')(X)\n",
    "\n",
    "    # Third component of main path\n",
    "    X = BatchNormalization(axis=3, name=bn_name_base + '2c')(X)\n",
    "    X = Activation('relu')(X)\n",
    "    X = Conv2D(F3, (1, 1), strides=(1, 1), padding='valid', name=conv_name_base + '2c')(X)\n",
    "\n",
    "    # Shortcut path\n",
    "    X_shortcut = Conv2D(F3, (1, 1), strides=(s, s), padding='valid', name=conv_name_base + '1')(X_shortcut)\n",
    "    X_shortcut = BatchNormalization(axis=3, name=bn_name_base + '1')(X_shortcut)\n",
    "\n",
    "    # Final step: Add shortcut value to main path\n",
    "    X = Add()([X, X_shortcut])\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom ResNet-50 v2 Model Architecture\n",
    "\n",
    "This section defines the architecture of a custom ResNet-50 v2 model with an additional stage (Stage 3.5). The model is implemented using TensorFlow and Keras, and it includes the following components:\n",
    "\n",
    "1. **Input Layer**:\n",
    "    - Accepts input images with a shape of `(224, 224, 3)`.\n",
    "\n",
    "2. **Stage 1**:\n",
    "    - Zero-padding is applied to the input.\n",
    "    - A convolutional layer with a kernel size of `(7, 7)` and stride `(2, 2)` is used.\n",
    "    - Batch normalization and ReLU activation are applied.\n",
    "    - Max pooling is performed to reduce spatial dimensions.\n",
    "\n",
    "3. **Stage 2**:\n",
    "    - A convolutional block is followed by two identity blocks.\n",
    "    - Each block uses filters `[64, 64, 256]`.\n",
    "\n",
    "4. **Stage 3**:\n",
    "    - A convolutional block is followed by three identity blocks.\n",
    "    - Each block uses filters `[128, 128, 512]`.\n",
    "\n",
    "5. **New Stage 3.5**:\n",
    "    - An additional stage is introduced to enhance feature extraction.\n",
    "    - A convolutional block is followed by two identity blocks.\n",
    "    - Each block uses filters `[128, 128, 512]`.\n",
    "\n",
    "6. **Stage 4**:\n",
    "    - A convolutional block is followed by five identity blocks.\n",
    "    - Each block uses filters `[256, 256, 1024]`.\n",
    "\n",
    "7. **Stage 5**:\n",
    "    - A convolutional block is followed by two identity blocks.\n",
    "    - Each block uses filters `[512, 512, 2048]`.\n",
    "\n",
    "8. **Average Pooling**:\n",
    "    - A global average pooling layer reduces the spatial dimensions to a single value per channel.\n",
    "\n",
    "9. **Output Layer**:\n",
    "    - A fully connected layer with a softmax activation function outputs predictions for `7` classes.\n",
    "\n",
    "The model is created using the `Model` API from Keras, and the function `ResNet50_v2` returns the complete model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, ZeroPadding2D, Conv2D, BatchNormalization, Activation, MaxPooling2D, AveragePooling2D, Flatten, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "def ResNet50_v2(input_shape=(224, 224, 3), classes=7):\n",
    "    # Define the input as a tensor with shape input_shape\n",
    "    X_input = Input(input_shape)\n",
    "\n",
    "    # Stage 1\n",
    "    X = ZeroPadding2D((3, 3))(X_input)\n",
    "    X = Conv2D(64, (7, 7), strides=(2, 2), name='conv1')(X)\n",
    "    X = BatchNormalization(axis=3, name='bn_conv1')(X)\n",
    "    X = Activation('relu')(X)\n",
    "    X = MaxPooling2D((3, 3), strides=(2, 2))(X)\n",
    "\n",
    "    # Stage 2\n",
    "    X = convolutional_block_v2(X, f=3, filters=[64, 64, 256], stage=2, block='a', s=1)\n",
    "    X = identity_block_v2(X, 3, [64, 64, 256], stage=2, block='b')\n",
    "    X = identity_block_v2(X, 3, [64, 64, 256], stage=2, block='c')\n",
    "\n",
    "    # Stage 3\n",
    "    X = convolutional_block_v2(X, f=3, filters=[128, 128, 512], stage=3, block='a', s=2)\n",
    "    X = identity_block_v2(X, 3, [128, 128, 512], stage=3, block='b')\n",
    "    X = identity_block_v2(X, 3, [128, 128, 512], stage=3, block='c')\n",
    "    X = identity_block_v2(X, 3, [128, 128, 512], stage=3, block='d')\n",
    "\n",
    "    # New Stage 3.5 (additional stage)\n",
    "    X = convolutional_block_v2(X, f=3, filters=[128,128,512], stage=3.5, block='a', s=2)\n",
    "    X = identity_block_v2(X, 3, [128,128,512], stage=3.5, block='b')\n",
    "    X = identity_block_v2(X, 3, [128,128,512], stage=3.5, block='c')\n",
    "\n",
    "    # Stage 4\n",
    "    X = convolutional_block_v2(X, f=3, filters=[256, 256, 1024], stage=4, block='a', s=2)\n",
    "    X = identity_block_v2(X, 3, [256, 256, 1024], stage=4, block='b')\n",
    "    X = identity_block_v2(X, 3, [256, 256, 1024], stage=4, block='c')\n",
    "    X = identity_block_v2(X, 3, [256, 256, 1024], stage=4, block='d')\n",
    "    X = identity_block_v2(X, 3, [256, 256, 1024], stage=4, block='e')\n",
    "    X = identity_block_v2(X, 3, [256, 256, 1024], stage=4, block='f')\n",
    "\n",
    "    # Stage 5\n",
    "    X = convolutional_block_v2(X, f=3, filters=[512, 512, 2048], stage=5, block='a', s=2)\n",
    "    X = identity_block_v2(X, 3, [512, 512, 2048], stage=5, block='b')\n",
    "    X = identity_block_v2(X, 3, [512, 512, 2048], stage=5, block='c')\n",
    "\n",
    "    # Average Pooling\n",
    "    X = AveragePooling2D(pool_size=(2, 2), padding='same')(X)\n",
    "\n",
    "    # Output layer\n",
    "    X = Flatten()(X)\n",
    "    X = Dense(classes, activation='softmax', name='fc' + str(classes))(X)\n",
    "\n",
    "    # Create model\n",
    "    model = Model(inputs=X_input, outputs=X, name='ResNet50_v2_stage')\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation, Dataset Preparation, and Model Training\n",
    "\n",
    "This section describes the process of augmenting the dataset, balancing class sizes, preparing training and validation datasets, and training the custom ResNet-50 v2 model.\n",
    "\n",
    "1. **Data Augmentation**:\n",
    "    - A function `augment_image` is defined to apply random transformations to images:\n",
    "        - Random horizontal and vertical flips.\n",
    "        - Random brightness and contrast adjustments.\n",
    "    - This helps improve the model's generalization by introducing variability in the training data.\n",
    "\n",
    "2. **Balancing Class Sizes**:\n",
    "    - The dataset is balanced by augmenting underrepresented classes.\n",
    "    - For each class:\n",
    "        - Images and labels are extracted.\n",
    "        - If the class size is smaller than the maximum class size, augmentation is applied, and the dataset is repeated to match the maximum size.\n",
    "\n",
    "3. **Dataset Preparation**:\n",
    "    - All class datasets are combined into a single balanced dataset.\n",
    "    - The dataset is shuffled and split into training and validation sets:\n",
    "        - 80% of the data is used for training.\n",
    "        - 20% of the data is used for validation.\n",
    "    - The datasets are batched and prefetched for efficient training.\n",
    "\n",
    "4. **Model Initialization**:\n",
    "    - A custom ResNet-50 v2 model with an additional stage is initialized using the `ResNet50_v2` function.\n",
    "    - The model summary is printed to display the number of parameters.\n",
    "\n",
    "5. **FLOPs Calculation**:\n",
    "    - A function `get_flops` is defined to calculate the number of floating-point operations (FLOPs) for the model.\n",
    "    - The FLOPs value is printed to assess the computational complexity of the model.\n",
    "\n",
    "6. **Model Compilation**:\n",
    "    - The model is compiled with:\n",
    "        - Adam optimizer (learning rate: 0.00001).\n",
    "        - Sparse categorical cross-entropy loss.\n",
    "        - Accuracy as the evaluation metric.\n",
    "\n",
    "7. **Callbacks**:\n",
    "    - Two callbacks are defined:\n",
    "        - `ModelCheckpoint`: Saves the best model based on validation loss.\n",
    "        - `ReduceLROnPlateau`: Reduces the learning rate if validation loss does not improve for 3 consecutive epochs.\n",
    "\n",
    "8. **Model Training**:\n",
    "    - The model is trained using the `fit` method with:\n",
    "        - Training and validation datasets.\n",
    "        - 25 epochs.\n",
    "        - Steps per epoch and validation steps calculated based on dataset size.\n",
    "        - The defined callbacks for saving the best model and learning rate scheduling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.framework.convert_to_constants import convert_variables_to_constants_v2\n",
    "\n",
    "# Determine the maximum number of images in any class\n",
    "label_counts = Counter(labels)\n",
    "max_count = max(label_counts.values())\n",
    "\n",
    "# Define a function for augmenting images\n",
    "def augment_image(image):\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    image = tf.image.random_flip_up_down(image)\n",
    "    image = tf.image.random_brightness(image, max_delta=0.1)\n",
    "    image = tf.image.random_contrast(image, lower=0.9, upper=1.1)\n",
    "    return image\n",
    "\n",
    "# Create datasets for each class with augmentation to balance the class sizes\n",
    "datasets = []\n",
    "for label, count in label_counts.items():\n",
    "    class_images = images[labels == label]\n",
    "    class_labels = labels[labels == label]\n",
    "    \n",
    "    class_dataset = tf.data.Dataset.from_tensor_slices((class_images, class_labels))\n",
    "    \n",
    "    # Apply augmentation if the class is underrepresented\n",
    "    if count < max_count:\n",
    "        class_dataset = class_dataset.map(lambda x, y: (augment_image(x), y))\n",
    "        class_dataset = class_dataset.repeat((max_count // count) + 1).take(max_count)\n",
    "    \n",
    "    datasets.append(class_dataset)\n",
    "\n",
    "# Combine all class datasets into one balanced dataset\n",
    "balanced_dataset = tf.data.Dataset.sample_from_datasets(datasets)\n",
    "\n",
    "# Shuffle the combined dataset\n",
    "balanced_dataset = balanced_dataset.shuffle(buffer_size=1000).repeat()\n",
    "\n",
    "# Split the balanced dataset into training and validation sets\n",
    "val_size = int(0.2 * max_count * len(label_counts))  # 20% of the total balanced dataset\n",
    "train_dataset = balanced_dataset.skip(val_size)\n",
    "val_dataset = balanced_dataset.take(val_size)\n",
    "\n",
    "# Define batch size\n",
    "batch_size = 32\n",
    "\n",
    "# Batch and prefetch the datasets\n",
    "train_dataset = train_dataset.batch(batch_size).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "val_dataset = val_dataset.batch(batch_size).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# Calculate steps per epoch\n",
    "steps_per_epoch = max(1, (max_count * len(label_counts) - val_size) // batch_size)\n",
    "validation_steps = max(1, val_size // batch_size)\n",
    "\n",
    "# Initialize the custom ResNet-50 v2 model\n",
    "num_classes = 7\n",
    "model = ResNet50_v2(input_shape=(256, 256, 3), classes=num_classes)\n",
    "# Print model summary to get number of parameters\n",
    "model.summary()\n",
    "\n",
    "# Function to calculate FLOPs\n",
    "def get_flops(model):\n",
    "    concrete_func = tf.function(lambda inputs: model(inputs))\n",
    "    concrete_func = concrete_func.get_concrete_function(\n",
    "        tf.TensorSpec([1] + list(model.input_shape[1:]))\n",
    "    )\n",
    "    frozen_func = convert_variables_to_constants_v2(concrete_func)\n",
    "    run_meta = tf.compat.v1.RunMetadata()\n",
    "    opts = tf.compat.v1.profiler.ProfileOptionBuilder.float_operation()    \n",
    "    flops = tf.compat.v1.profiler.profile(graph=frozen_func.graph, run_meta=run_meta, options=opts)\n",
    "    \n",
    "    return flops.total_float_ops\n",
    "\n",
    "# Get FLOPs and print\n",
    "flops = get_flops(model)\n",
    "print(f\"FLOPs: {flops}\")\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.00001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Define callbacks\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.ModelCheckpoint(\"resnet-50-v2_custom_stage.keras\", save_best_only=True, monitor=\"val_loss\")\n",
    "]\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-7)\n",
    "history = model.fit(train_dataset, \n",
    "                    validation_data=val_dataset, \n",
    "                    epochs=25, \n",
    "                    steps_per_epoch=steps_per_epoch, \n",
    "                    validation_steps=validation_steps, \n",
    "                    callbacks=[lr_scheduler])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is used to **visualize the training and validation performance of the model** over the epochs. Specifically:\n",
    "\n",
    "1. **Left Plot**: Displays the training and validation accuracy values for each epoch, helping to assess how well the model is learning and generalizing.\n",
    "2. **Right Plot**: Shows the training and validation loss values for each epoch, providing insights into the model's convergence and overfitting behavior.\n",
    "\n",
    "These plots are essential for monitoring the model's performance during training and identifying potential issues like overfitting or underfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. **Save the Final Model**:\n",
    "    - The trained ResNet-50 v2 model is saved to a file named `\"resnet_50-v2.keras\"` for future use.\n",
    "\n",
    "2. **Import Required Libraries**:\n",
    "    - Libraries like `numpy`, `matplotlib.pyplot`, and `sklearn` are imported for metrics computation and visualization.\n",
    "\n",
    "3. **Define a Function to Get True Labels and Predicted Probabilities**:\n",
    "    - The function `get_true_labels_and_probs` iterates over the validation dataset to collect the true labels and predicted probabilities from the model.\n",
    "\n",
    "4. **Compute True Labels and Predicted Probabilities**:\n",
    "    - The function is used to get the true labels and predicted probabilities for the validation dataset.\n",
    "\n",
    "5. **Binarize True Labels**:\n",
    "    - The true labels are converted into a binary format using `to_categorical` for multi-class ROC computation.\n",
    "\n",
    "6. **Calculate and Plot Confusion Matrix**:\n",
    "    - A confusion matrix is computed using `confusion_matrix` and visualized using `ConfusionMatrixDisplay`.\n",
    "\n",
    "7. **Plot ROC Curve for Each Class**:\n",
    "    - The ROC curve is plotted for each class using `roc_curve` and `auc`. The Area Under the Curve (AUC) is also displayed for each class.\n",
    "\n",
    "This code is primarily used for **model evaluation and visualization**, focusing on the confusion matrix and ROC curves for multi-class classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Function to get true labels and predicted labels from the model\n",
    "def get_true_and_predicted_labels(model, dataset):\n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "    for x, y_true in dataset:\n",
    "        y_pred = np.argmax(model.predict(x), axis=-1)\n",
    "        true_labels.extend(y_true.numpy())\n",
    "        predicted_labels.extend(y_pred)\n",
    "    return true_labels, predicted_labels\n",
    "\n",
    "# Get true and predicted labels for validation dataset\n",
    "true_labels, predicted_labels = get_true_and_predicted_labels(model, val_dataset)\n",
    "\n",
    "# Calculate confusion matrix\n",
    "cm = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=range(num_classes))\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation Metrics\n",
    "\n",
    "This section focuses on evaluating the performance of the trained ResNet-50 v2 model using various metrics. The following steps are performed:\n",
    "\n",
    "1. **Accuracy**: Measures the overall correctness of the model's predictions.\n",
    "2. **Precision (Macro)**: Evaluates the ability of the model to avoid false positives across all classes.\n",
    "3. **Recall (Macro)**: Measures the ability of the model to identify all relevant instances across all classes.\n",
    "4. **F1-Score (Macro)**: Provides a balance between precision and recall.\n",
    "5. **Confusion Matrix**: Displays the performance of the model in a tabular format, showing true positives, false positives, true negatives, and false negatives for each class.\n",
    "6. **MCC (Matthews Correlation Coefficient)**: A balanced measure that accounts for true and false positives and negatives.\n",
    "7. **Cohen's Kappa**: Measures the agreement between predicted and true labels, adjusted for chance.\n",
    "8. **Hamming Loss**: Calculates the fraction of incorrect labels.\n",
    "9. **Jaccard Score (Macro)**: Measures the similarity between predicted and true labels.\n",
    "\n",
    "These metrics provide a comprehensive evaluation of the model's performance, ensuring that it performs well across multiple dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score, log_loss, matthews_corrcoef, cohen_kappa_score, hamming_loss, jaccard_score\n",
    "y_true = true_labels\n",
    "y_pred = predicted_labels\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision_macro = precision_score(y_true, y_pred, average='macro')\n",
    "recall_macro = recall_score(y_true, y_pred, average='macro')\n",
    "f1_macro = f1_score(y_true, y_pred, average='macro')\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "mcc = matthews_corrcoef(y_true, y_pred)\n",
    "kappa = cohen_kappa_score(y_true, y_pred)\n",
    "hamming = hamming_loss(y_true, y_pred)\n",
    "jaccard_macro = jaccard_score(y_true, y_pred, average='macro')\n",
    "\n",
    "# Output the metrics\n",
    "metrics = {\n",
    "    \"Accuracy\": accuracy,\n",
    "    \"Precision (Macro)\": precision_macro,\n",
    "    \"Recall (Macro)\": recall_macro,\n",
    "    \"F1-Score (Macro)\": f1_macro,\n",
    "    \"Confusion Matrix\": cm,\n",
    "    \"MCC\": mcc,\n",
    "    \"Cohen's Kappa\": kappa,\n",
    "    \"Hamming Loss\": hamming,\n",
    "    \"Jaccard Score (Macro)\": jaccard_macro\n",
    "}\n",
    "\n",
    "for metric, value in metrics.items():\n",
    "    print(f\"{metric}: {value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Training and Validation Metrics\n",
    "\n",
    "The following code extracts the training and validation metrics (loss and accuracy) from the `history` object and saves them into a CSV file for further analysis:\n",
    "\n",
    "1. **Extract Metrics**:\n",
    "    - `train_loss` and `train_accuracy` are extracted for the training dataset.\n",
    "    - `val_loss` and `val_accuracy` are extracted for the validation dataset.\n",
    "\n",
    "2. **Create a DataFrame**:\n",
    "    - A pandas DataFrame is created to organize the metrics into columns: `Train Loss`, `Train Accuracy`, `Validation Loss`, and `Validation Accuracy`.\n",
    "\n",
    "3. **Save to CSV**:\n",
    "    - The DataFrame is saved to a file named `Resnet-50-custom.csv`, which can be used for visualization or reporting purposes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Extract all metrics from history\n",
    "train_loss = history.history['loss']\n",
    "train_accuracy = history.history['accuracy']\n",
    "val_loss = history.history['val_loss']\n",
    "val_accuracy = history.history['val_accuracy']\n",
    "\n",
    "# Create a DataFrame to store all metrics\n",
    "metrics_df = pd.DataFrame({\n",
    "    'Train Loss': train_loss,\n",
    "    'Train Accuracy': train_accuracy,\n",
    "    'Validation Loss': val_loss,\n",
    "    'Validation Accuracy': val_accuracy\n",
    "})\n",
    "\n",
    "# Save metrics to CSV\n",
    "metrics_df.to_csv('Resnet-50-custom.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
